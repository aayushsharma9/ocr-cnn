python3.7 ocrtrain2.py
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
(592713, 784)
(592713, 28, 28, 1)
(592713, 28, 28, 1)
Name: 5-conv-128-nodes-3-dense1567868963
WARNING: Logging before flag parsing goes to stderr.
W0907 20:39:23.259089 140045871539840 deprecation.py:506] From /usr/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 28, 28, 32)        320       
_________________________________________________________________
batch_normalization (BatchNo (None, 28, 28, 32)        128       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 26, 26, 32)        9248      
_________________________________________________________________
batch_normalization_1 (Batch (None, 26, 26, 32)        128       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 13, 13, 32)        25632     
_________________________________________________________________
batch_normalization_2 (Batch (None, 13, 13, 32)        128       
_________________________________________________________________
dropout (Dropout)            (None, 13, 13, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     
_________________________________________________________________
batch_normalization_3 (Batch (None, 11, 11, 64)        256       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 9, 9, 64)          36928     
_________________________________________________________________
batch_normalization_4 (Batch (None, 9, 9, 64)          256       
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 5, 5, 64)          102464    
_________________________________________________________________
batch_normalization_5 (Batch (None, 5, 5, 64)          256       
_________________________________________________________________
dropout_1 (Dropout)          (None, 5, 5, 64)          0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 2, 2, 128)         131200    
_________________________________________________________________
batch_normalization_6 (Batch (None, 2, 2, 128)         512       
_________________________________________________________________
flatten (Flatten)            (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense (Dense)                (None, 62)                31806     
=================================================================
Total params: 357,758
Trainable params: 356,926
Non-trainable params: 832
_________________________________________________________________
Train on 474170 samples, validate on 118543 samples
2019-09-07 20:39:24.819596: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-07 20:39:24.841413: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2593000000 Hz
2019-09-07 20:39:24.841836: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fa7d7451f0 executing computations on platform Host. Devices:
2019-09-07 20:39:24.841863: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-09-07 20:39:25.013143: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
Epoch 1/20
2019-09-07 20:39:25.822473: W tensorflow/core/framework/allocator.cc:107] Allocation of 17981568 exceeds 10% of system memory.
2019-09-07 20:39:25.822473: W tensorflow/core/framework/allocator.cc:107] Allocation of 17981568 exceeds 10% of system memory.
2019-09-07 20:39:25.828327: W tensorflow/core/framework/allocator.cc:107] Allocation of 25958400 exceeds 10% of system memory.
2019-09-07 20:39:25.828327: W tensorflow/core/framework/allocator.cc:107] Allocation of 25958400 exceeds 10% of system memory.
2019-09-07 20:39:25.853634: W tensorflow/core/framework/allocator.cc:107] Allocation of 27256320 exceeds 10% of system memory.
2019-09-07 20:39:25.913376: I tensorflow/core/profiler/lib/profiler_session.cc:174] Profiler session started.
474170/474170 [==============================] - 1386s 3ms/sample - loss: 0.5548 - acc: 0.8285 - val_loss: 0.3889 - val_acc: 0.8696
Epoch 2/20
474170/474170 [==============================] - 1423s 3ms/sample - loss: 0.3696 - acc: 0.8784 - val_loss: 0.4347 - val_acc: 0.8544
Epoch 3/20
474170/474170 [==============================] - 1441s 3ms/sample - loss: 0.3371 - acc: 0.8881 - val_loss: 0.2904 - val_acc: 0.9020
Epoch 4/20
474170/474170 [==============================] - 1387s 3ms/sample - loss: 0.3192 - acc: 0.8934 - val_loss: 0.2842 - val_acc: 0.9040
Epoch 5/20
474170/474170 [==============================] - 1330s 3ms/sample - loss: 0.3071 - acc: 0.8971 - val_loss: 0.2765 - val_acc: 0.9059
Epoch 6/20
474170/474170 [==============================] - 1429s 3ms/sample - loss: 0.3000 - acc: 0.8993 - val_loss: 0.2718 - val_acc: 0.9080
Epoch 7/20
474170/474170 [==============================] - 1330s 3ms/sample - loss: 0.2926 - acc: 0.9011 - val_loss: 0.2756 - val_acc: 0.9079
Epoch 8/20
474170/474170 [==============================] - 1379s 3ms/sample - loss: 0.2877 - acc: 0.9027 - val_loss: 0.2643 - val_acc: 0.9121
Epoch 9/20
474170/474170 [==============================] - 1330s 3ms/sample - loss: 0.2837 - acc: 0.9042 - val_loss: 0.2586 - val_acc: 0.9130
Epoch 10/20
474170/474170 [==============================] - 1328s 3ms/sample - loss: 0.2796 - acc: 0.9053 - val_loss: 0.3490 - val_acc: 0.8806
Epoch 11/20
474170/474170 [==============================] - 1420s 3ms/sample - loss: 0.2755 - acc: 0.9066 - val_loss: 0.2566 - val_acc: 0.9143
Epoch 12/20
474170/474170 [==============================] - 1328s 3ms/sample - loss: 0.2737 - acc: 0.9068 - val_loss: 0.2630 - val_acc: 0.9109
Epoch 13/20
474170/474170 [==============================] - 1328s 3ms/sample - loss: 0.2708 - acc: 0.9084 - val_loss: 0.2536 - val_acc: 0.9142
Epoch 14/20
474170/474170 [==============================] - 1328s 3ms/sample - loss: 0.2683 - acc: 0.9087 - val_loss: 0.2556 - val_acc: 0.9134
Epoch 15/20
474170/474170 [==============================] - 1329s 3ms/sample - loss: 0.2659 - acc: 0.9087 - val_loss: 0.2688 - val_acc: 0.9084
Epoch 16/20
474170/474170 [==============================] - 1327s 3ms/sample - loss: 0.2638 - acc: 0.9097 - val_loss: 0.2512 - val_acc: 0.9152
Epoch 17/20
474170/474170 [==============================] - 1330s 3ms/sample - loss: 0.2625 - acc: 0.9099 - val_loss: 0.2527 - val_acc: 0.9151
Epoch 18/20
474170/474170 [==============================] - 1325s 3ms/sample - loss: 0.2607 - acc: 0.9105 - val_loss: 0.2535 - val_acc: 0.9152
Epoch 19/20
474170/474170 [==============================] - 1328s 3ms/sample - loss: 0.2601 - acc: 0.9106 - val_loss: 0.2583 - val_acc: 0.9122
Epoch 20/20
474170/474170 [==============================] - 1329s 3ms/sample - loss: 0.2580 - acc: 0.9114 - val_loss: 0.2464 - val_acc: 0.9163
142700/142700 [==============================] - 83s 581us/sample - loss: 0.4932 - acc: 0.8113