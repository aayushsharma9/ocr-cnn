python3.7 ocrtrain2.py
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
(592713, 784)
(592713, 28, 28, 1)
(592713, 28, 28, 1)
Name: 5-conv-128-nodes-3-dense1567839334
WARNING: Logging before flag parsing goes to stderr.
W0907 12:25:34.682798 140157381318272 deprecation.py:506] From /usr/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 28, 28, 32)        320       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         
_________________________________________________________________
batch_normalization (BatchNo (None, 14, 14, 32)        128       
_________________________________________________________________
flatten (Flatten)            (None, 6272)              0         
_________________________________________________________________
dense (Dense)                (None, 512)               3211776   
_________________________________________________________________
dense_1 (Dense)              (None, 384)               196992    
_________________________________________________________________
dense_2 (Dense)              (None, 256)               98560     
_________________________________________________________________
dense_3 (Dense)              (None, 192)               49344     
_________________________________________________________________
dense_4 (Dense)              (None, 128)               24704     
_________________________________________________________________
dense_5 (Dense)              (None, 62)                7998      
=================================================================
Total params: 3,589,822
Trainable params: 3,589,758
Non-trainable params: 64
_________________________________________________________________
Train on 474170 samples, validate on 118543 samples
2019-09-07 12:25:35.291609: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-07 12:25:35.311069: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2593000000 Hz
2019-09-07 12:25:35.311489: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564fe0f569e0 executing computations on platform Host. Devices:
2019-09-07 12:25:35.311520: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-09-07 12:25:35.398280: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
Epoch 1/50
2019-09-07 12:25:35.797915: I tensorflow/core/profiler/lib/profiler_session.cc:174] Profiler session started.
474170/474170 [==============================] - 518s 1ms/sample - loss: 0.5454 - acc: 0.8358 - val_loss: 0.4334 - val_acc: 0.8649
Epoch 2/50
474170/474170 [==============================] - 459s 969us/sample - loss: 0.3912 - acc: 0.8781 - val_loss: 0.3897 - val_acc: 0.8772
Epoch 3/50
474170/474170 [==============================] - 436s 921us/sample - loss: 0.3508 - acc: 0.8889 - val_loss: 0.3644 - val_acc: 0.8869
Epoch 4/50
474170/474170 [==============================] - 500s 1ms/sample - loss: 0.3283 - acc: 0.8949 - val_loss: 0.3969 - val_acc: 0.8854
Epoch 5/50
474170/474170 [==============================] - 507s 1ms/sample - loss: 0.3352 - acc: 0.8960 - val_loss: 0.3795 - val_acc: 0.8866
Epoch 6/50
474170/474170 [==============================] - 458s 967us/sample - loss: 0.3097 - acc: 0.9001 - val_loss: 0.3795 - val_acc: 0.8901
Epoch 7/50
474170/474170 [==============================] - 431s 910us/sample - loss: 0.3148 - acc: 0.9006 - val_loss: 0.3859 - val_acc: 0.8898
Epoch 8/50
474170/474170 [==============================] - 432s 910us/sample - loss: 0.3150 - acc: 0.9010 - val_loss: 0.3785 - val_acc: 0.8898
Epoch 9/50
474170/474170 [==============================] - 478s 1ms/sample - loss: 0.3040 - acc: 0.9038 - val_loss: 0.5910 - val_acc: 0.8922
Epoch 10/50
474170/474170 [==============================] - 468s 988us/sample - loss: 0.3094 - acc: 0.9038 - val_loss: 0.4402 - val_acc: 0.8883
Epoch 11/50
474170/474170 [==============================] - 431s 909us/sample - loss: 0.3163 - acc: 0.9026 - val_loss: 0.4500 - val_acc: 0.8887
Epoch 12/50
474170/474170 [==============================] - 431s 909us/sample - loss: 0.3034 - acc: 0.9055 - val_loss: 0.3936 - val_acc: 0.8926
Epoch 13/50
474170/474170 [==============================] - 422s 890us/sample - loss: 0.3225 - acc: 0.9037 - val_loss: 0.4074 - val_acc: 0.8865
Epoch 14/50
474170/474170 [==============================] - 421s 887us/sample - loss: 0.2995 - acc: 0.9060 - val_loss: 0.4139 - val_acc: 0.8918
Epoch 15/50
474170/474170 [==============================] - 421s 887us/sample - loss: 0.3008 - acc: 0.9061 - val_loss: 0.4331 - val_acc: 0.8892
Epoch 16/50
474170/474170 [==============================] - 420s 887us/sample - loss: 0.3132 - acc: 0.9040 - val_loss: 0.4310 - val_acc: 0.8863
Epoch 17/50
474170/474170 [==============================] - 421s 887us/sample - loss: 0.3106 - acc: 0.9049 - val_loss: 0.4409 - val_acc: 0.8860
Epoch 18/50
474170/474170 [==============================] - 486s 1ms/sample - loss: 0.3246 - acc: 0.9026 - val_loss: 0.4576 - val_acc: 0.8842
Epoch 19/50
474170/474170 [==============================] - 481s 1ms/sample - loss: 0.3304 - acc: 0.9008 - val_loss: 0.5132 - val_acc: 0.8587
Epoch 20/50
474170/474170 [==============================] - 460s 970us/sample - loss: 0.3352 - acc: 0.8999 - val_loss: 0.4704 - val_acc: 0.8876
Epoch 21/50
474170/474170 [==============================] - 431s 909us/sample - loss: 0.3620 - acc: 0.8961 - val_loss: 0.4798 - val_acc: 0.8841
Epoch 22/50
474170/474170 [==============================] - 487s 1ms/sample - loss: 0.3710 - acc: 0.8993 - val_loss: 0.5026 - val_acc: 0.8760
Epoch 23/50
474170/474170 [==============================] - 421s 888us/sample - loss: 0.3526 - acc: 0.8982 - val_loss: 0.5538 - val_acc: 0.8880
Epoch 24/50
474170/474170 [==============================] - 420s 886us/sample - loss: 0.3538 - acc: 0.8980 - val_loss: 0.5000 - val_acc: 0.8676
Epoch 25/50
474170/474170 [==============================] - 420s 886us/sample - loss: 0.3590 - acc: 0.8955 - val_loss: 0.4686 - val_acc: 0.8752
Epoch 26/50
474170/474170 [==============================] - 420s 885us/sample - loss: 0.3669 - acc: 0.8938 - val_loss: 0.4669 - val_acc: 0.8851
Epoch 27/50
474170/474170 [==============================] - 421s 887us/sample - loss: 0.3746 - acc: 0.8931 - val_loss: 0.4754 - val_acc: 0.8783
Epoch 28/50
474170/474170 [==============================] - 420s 885us/sample - loss: 0.3862 - acc: 0.8931 - val_loss: 0.6994 - val_acc: 0.8334
Epoch 29/50
474170/474170 [==============================] - 420s 886us/sample - loss: 0.3770 - acc: 0.8926 - val_loss: 0.5159 - val_acc: 0.8821
Epoch 30/50
474170/474170 [==============================] - 422s 890us/sample - loss: 0.3835 - acc: 0.8937 - val_loss: 0.4587 - val_acc: 0.8877
Epoch 31/50
474170/474170 [==============================] - 420s 886us/sample - loss: 0.3909 - acc: 0.8919 - val_loss: 0.4995 - val_acc: 0.8811
Epoch 32/50
474170/474170 [==============================] - 420s 886us/sample - loss: 0.3998 - acc: 0.8889 - val_loss: 0.5507 - val_acc: 0.8832
Epoch 33/50
474170/474170 [==============================] - 420s 886us/sample - loss: 0.3904 - acc: 0.8908 - val_loss: 0.5396 - val_acc: 0.8718
Epoch 34/50
474170/474170 [==============================] - 420s 887us/sample - loss: 0.4012 - acc: 0.8895 - val_loss: 0.5162 - val_acc: 0.8839
Epoch 35/50
474170/474170 [==============================] - 467s 985us/sample - loss: 0.4097 - acc: 0.8891 - val_loss: 0.6245 - val_acc: 0.8801
Epoch 36/50
474170/474170 [==============================] - 455s 959us/sample - loss: 0.4186 - acc: 0.8864 - val_loss: 0.6746 - val_acc: 0.8536
Epoch 37/50
474170/474170 [==============================] - 420s 885us/sample - loss: 0.4329 - acc: 0.8814 - val_loss: 0.5461 - val_acc: 0.8703
Epoch 38/50
474170/474170 [==============================] - 421s 888us/sample - loss: 0.4636 - acc: 0.8790 - val_loss: 0.5770 - val_acc: 0.8808
Epoch 39/50
474170/474170 [==============================] - 419s 884us/sample - loss: 0.4162 - acc: 0.8869 - val_loss: 0.6353 - val_acc: 0.8831
Epoch 40/50
474170/474170 [==============================] - 420s 887us/sample - loss: 0.4876 - acc: 0.8786 - val_loss: 0.5464 - val_acc: 0.8804
Epoch 41/50
474170/474170 [==============================] - 420s 887us/sample - loss: 0.4221 - acc: 0.8862 - val_loss: 0.5752 - val_acc: 0.8747
Epoch 42/50
474170/474170 [==============================] - 420s 885us/sample - loss: 0.3937 - acc: 0.8909 - val_loss: 0.5191 - val_acc: 0.8826
Epoch 43/50
474170/474170 [==============================] - 452s 954us/sample - loss: 0.4027 - acc: 0.8895 - val_loss: 0.5476 - val_acc: 0.8848
Epoch 44/50
474170/474170 [==============================] - 530s 1ms/sample - loss: 0.4081 - acc: 0.8895 - val_loss: 0.5645 - val_acc: 0.8672
Epoch 45/50
474170/474170 [==============================] - 531s 1ms/sample - loss: 0.4152 - acc: 0.8888 - val_loss: 0.5668 - val_acc: 0.8797
Epoch 46/50
474170/474170 [==============================] - 490s 1ms/sample - loss: 0.4599 - acc: 0.8873 - val_loss: 0.5761 - val_acc: 0.8768
Epoch 47/50
474170/474170 [==============================] - 448s 945us/sample - loss: 0.3989 - acc: 0.8909 - val_loss: 0.5133 - val_acc: 0.8849
Epoch 48/50
474170/474170 [==============================] - 421s 887us/sample - loss: 0.4687 - acc: 0.8823 - val_loss: 0.5960 - val_acc: 0.8587
Epoch 49/50
474170/474170 [==============================] - 420s 885us/sample - loss: 0.4003 - acc: 0.8909 - val_loss: 0.5730 - val_acc: 0.8878
Epoch 50/50
474170/474170 [==============================] - 420s 885us/sample - loss: 0.3939 - acc: 0.8927 - val_loss: 0.5813 - val_acc: 0.8680
142700/142700 [==============================] - 29s 200us/sample - loss: 0.8620 - acc: 0.7681