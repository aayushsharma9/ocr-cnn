
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
(592713, 784)
(592713, 28, 28, 1)
(592713, 28, 28, 1)
WARNING: Logging before flag parsing goes to stderr.
W0905 18:44:11.175580 140027024574080 deprecation.py:506] From /usr/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 28, 28, 32)        320       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         
_________________________________________________________________
batch_normalization (BatchNo (None, 14, 14, 32)        128       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 7, 7, 64)          256       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 3, 3, 64)          0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 3, 3, 64)          256       
_________________________________________________________________
dropout (Dropout)            (None, 3, 3, 64)          0         
_________________________________________________________________
flatten (Flatten)            (None, 576)               0         
_________________________________________________________________
dense (Dense)                (None, 256)               147712    
_________________________________________________________________
dense_1 (Dense)              (None, 128)               32896     
_________________________________________________________________
dense_2 (Dense)              (None, 62)                7998      
=================================================================
Total params: 244,990
Trainable params: 244,670
Non-trainable params: 320
_________________________________________________________________
Train on 414899 samples, validate on 177814 samples
2019-09-05 18:44:12.121414: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-05 18:44:12.154865: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2593000000 Hz
2019-09-05 18:44:12.155436: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562fba99ef90 executing computations on platform Host. Devices:
2019-09-05 18:44:12.155475: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-09-05 18:44:12.280696: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
Epoch 1/20
2019-09-05 18:44:12.828297: W tensorflow/core/framework/allocator.cc:107] Allocation of 20321280 exceeds 10% of system memory.
2019-09-05 18:44:12.828297: W tensorflow/core/framework/allocator.cc:107] Allocation of 20321280 exceeds 10% of system memory.
2019-09-05 18:44:12.861547: I tensorflow/core/profiler/lib/profiler_session.cc:174] Profiler session started.
    64/414899 [..............................] - ETA: 47:48 - loss: 4.3869 - acc: 0.0000e+002019-09-05 18:44:12.912414: W tensorflow/core/framework/allocator.cc:107] Allocation of 20321280 exceeds 10% of system memory.
2019-09-05 18:44:12.912505: W tensorflow/core/framework/allocator.cc:107] Allocation of 20321280 exceeds 10% of system memory.
   128/414899 [..............................] - ETA: 28:30 - loss: 4.2959 - acc: 0.0391    2019-09-05 18:44:12.989634: W tensorflow/core/framework/allocator.cc:107] Allocation of 20321280 exceeds 10% of system memory.
2019-09-05 18:44:12.989634: W tensorflow/core/framework/allocator.cc:107] Allocation of 20321280 exceeds 10% of system memory.
414899/414899 [==============================] - 428s 1ms/sample - loss: 0.4859 - acc: 0.8471 - val_loss: 0.3577 - val_acc: 0.8835
Epoch 2/20
414899/414899 [==============================] - 386s 931us/sample - loss: 0.3462 - acc: 0.8846 - val_loss: 0.3238 - val_acc: 0.8947
Epoch 3/20
414899/414899 [==============================] - 345s 832us/sample - loss: 0.3139 - acc: 0.8938 - val_loss: 0.3171 - val_acc: 0.8942
Epoch 4/20
414899/414899 [==============================] - 345s 831us/sample - loss: 0.2930 - acc: 0.8995 - val_loss: 0.3014 - val_acc: 0.9009
Epoch 5/20
414899/414899 [==============================] - 344s 830us/sample - loss: 0.2791 - acc: 0.9039 - val_loss: 0.2932 - val_acc: 0.9021
Epoch 6/20
414899/414899 [==============================] - 345s 830us/sample - loss: 0.2684 - acc: 0.9073 - val_loss: 0.2913 - val_acc: 0.9022
Epoch 7/20
414899/414899 [==============================] - 345s 831us/sample - loss: 0.2598 - acc: 0.9096 - val_loss: 0.2822 - val_acc: 0.9054
Epoch 8/20
414899/414899 [==============================] - 345s 830us/sample - loss: 0.2522 - acc: 0.9115 - val_loss: 0.2860 - val_acc: 0.9049
Epoch 9/20
414899/414899 [==============================] - 347s 836us/sample - loss: 0.2463 - acc: 0.9136 - val_loss: 0.2931 - val_acc: 0.9027
Epoch 10/20
414899/414899 [==============================] - 344s 830us/sample - loss: 0.2417 - acc: 0.9142 - val_loss: 0.2807 - val_acc: 0.9076
Epoch 11/20
414899/414899 [==============================] - 344s 829us/sample - loss: 0.2367 - acc: 0.9157 - val_loss: 0.2841 - val_acc: 0.9073
Epoch 12/20
414899/414899 [==============================] - 344s 829us/sample - loss: 0.2326 - acc: 0.9172 - val_loss: 0.2795 - val_acc: 0.9089
Epoch 13/20
414899/414899 [==============================] - 345s 831us/sample - loss: 0.2290 - acc: 0.9182 - val_loss: 0.2852 - val_acc: 0.9074
Epoch 14/20
414899/414899 [==============================] - 345s 831us/sample - loss: 0.2257 - acc: 0.9192 - val_loss: 0.2812 - val_acc: 0.9090
Epoch 15/20
414899/414899 [==============================] - 345s 831us/sample - loss: 0.2228 - acc: 0.9200 - val_loss: 0.2832 - val_acc: 0.9064
Epoch 16/20
414899/414899 [==============================] - 344s 830us/sample - loss: 0.2194 - acc: 0.9208 - val_loss: 0.2846 - val_acc: 0.9084
Epoch 17/20
414899/414899 [==============================] - 345s 831us/sample - loss: 0.2178 - acc: 0.9213 - val_loss: 0.2856 - val_acc: 0.9080
Epoch 18/20
414899/414899 [==============================] - 344s 829us/sample - loss: 0.2148 - acc: 0.9225 - val_loss: 0.2902 - val_acc: 0.9072
Epoch 19/20
414899/414899 [==============================] - 345s 831us/sample - loss: 0.2134 - acc: 0.9229 - val_loss: 0.2874 - val_acc: 0.9080
Epoch 20/20
414899/414899 [==============================] - 344s 830us/sample - loss: 0.2116 - acc: 0.9230 - val_loss: 0.2895 - val_acc: 0.9076
142700/142700 [==============================] - 31s 217us/sample - loss: 0.5691 - acc: 0.7945